{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **gradient** is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. \n",
    "- We call our process **gradient descent** because it uses the gradient to descend the loss curve towards a minimum. \n",
    "- **Stochastic** means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample code\n",
    "'''\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type=\"invscaling\"):\n",
    "    \"\"\" Computes Ordinary Least SquaresLinear Regression with Stochastic Gradient Descent as the optimization algorithm.\n",
    "        :param feature_array: array with all feature vectors used to train the model\n",
    "        :param target_array: array with all target vectors used to train the model\n",
    "        :param to_predict: feature vector that is not contained in the training set. Used to make a new prediction\n",
    "        :param learn_rate_type: algorithm used to set the learning rate at each iteration.\n",
    "        :return: Predicted cooking time for the vector to_predict and the R-squared of the model.\n",
    "\"\"\"    # Pipeline of transformations to apply to an estimator. First applies Standard Scaling to the feature array.\n",
    "    # Then, when the model is fitting the data it runs Stochastic Gradient Descent as the optimization algorithm.\n",
    "    # The estimator is always the last element.\n",
    "    \n",
    "    start_time = time.time()\n",
    "    linear_regression_pipeline = make_pipeline(StandardScaler(), SGDRegressor(learning_rate=learn_rate_type))\n",
    "    \n",
    "    linear_regression_pipeline.fit(feature_array, target_array)\n",
    "    stop_time = time.time()\n",
    "     \n",
    "    print(\"Total runtime: %.6fs\" % (stop_time - start_time))\n",
    "    print(\"Algorithm used to set the learning rate: \" + learn_rate_type)\n",
    "    print(\"Model Coeffiecients: \" + str(linear_regression_pipeline[1].coef_))\n",
    "    print(\"Number of iterations: \" + str(linear_regression_pipeline[1].n_iter_))    # Make a prediction for a feature vector not in the training set\n",
    "    prediction = np.round(linear_regression_pipeline.predict(to_predict), 0)[0]\n",
    "    print(\"Predicted cooking time: \" + str(prediction) + \" minutes\")    \n",
    "    r_squared = np.round(linear_regression_pipeline.score(feature_array, target_array).reshape(-1, 1)[0][0], 2)\n",
    "    print(\"R-squared: \" + str(r_squared))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = [[500, 80, 30, 10],\n",
    "                 [550, 75, 25, 0],\n",
    "                 [475, 90, 35, 20],\n",
    "                 [450, 80, 20,25],\n",
    "                 [465, 75, 30, 0],\n",
    "                 [525, 65, 40, 15],\n",
    "                 [400, 85, 33, 0],\n",
    "                 [500, 60, 30, 30],\n",
    "                 [435, 45, 25, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_array = [17, 11, 21, 23, 22, 15, 25, 18, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = [[510, 50, 35, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.001998s\n",
      "Algorithm used to set the learning rate: invscaling\n",
      "Model Coeffiecients: [-3.44034236  1.64723444  0.28599174  1.10821407]\n",
      "Number of iterations: 249\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.9\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.002000s\n",
      "Algorithm used to set the learning rate: invscaling\n",
      "Model Coeffiecients: [-3.44226981  1.64672679  0.2866046   1.10902962]\n",
      "Number of iterations: 248\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.9\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.001999s\n",
      "Algorithm used to set the learning rate: adaptive\n",
      "Model Coeffiecients: [-3.49884884  1.64454487  0.3091154   1.15922034]\n",
      "Number of iterations: 97\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type=\"adaptive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "With the limitations of Gradient Descent in mind, Stochastic Gradient Descent emerged as a way to tackle performance issues and speed up the convergence in large datasets.\n",
    "\n",
    "Stochastic Gradient Descent is a probabilistic approximation of Gradient Descent. It is an approximation because, at each step, the algorithm calculates the gradient for one observation picked at random, instead of calculating the gradient for the entire dataset.\n",
    "'''\n",
    "src: https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loop(runs=3):\n",
    "    \"\"\" Repeatedly computes the gradient of a function\n",
    "        Computes the gradient given the starting points and then uses the result of the gradient to feed the next iteration, with new points.\n",
    "        Prints out the result of the function at each iteration\n",
    "        :param: runs: number of iterations to compute\n",
    "    \"\"\"    # starting points\n",
    "    x = np.array([1, 2, 3])\n",
    "    \n",
    "    # quadratic function, a parabola\n",
    "    y = x**2\n",
    "    \n",
    "    for run in range(0, runs):\n",
    "        print(\"Iter \" + str(run) + \": Y=\" + str(y))        # compute first derivative\n",
    "        x = np.gradient(y, 1)        # update the function output\n",
    "        y = x ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Y=[1 4 9]\n",
      "Iter 1: Y=[ 9. 16. 25.]\n",
      "Iter 2: Y=[49. 64. 81.]\n",
      "Iter 3: Y=[225. 256. 289.]\n",
      "Iter 4: Y=[ 961. 1024. 1089.]\n",
      "Iter 5: Y=[3969. 4096. 4225.]\n",
      "Iter 6: Y=[16129. 16384. 16641.]\n",
      "Iter 7: Y=[65025. 65536. 66049.]\n",
      "Iter 8: Y=[261121. 262144. 263169.]\n",
      "Iter 9: Y=[1046529. 1048576. 1050625.]\n"
     ]
    }
   ],
   "source": [
    "gradient_loop(runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Neural Network-Based SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the training data we have, we need two more things:\n",
    "- A **loss function** that measures how good the network's predictions are. it measures the disparity between the the target's true value and the value the model predicts.\n",
    "  -- A common loss function for regression problems is the **mean absolute error or MAE**. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
    "- An **optimizer** that can tell the network how to change its weights. The optimizer is an algorithm that adjusts the weights to minimize the loss. \n",
    "  -- Virtually all of the optimization algorithms used in deep learning belong to a family called **stochastic gradient descent.**  They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "\n",
    "    * Sample some training data and run it through the network to make predictions.\n",
    "    * Measure the loss between the predictions and the true values.\n",
    "    * Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/batch-sgd.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='img/batch-sgd.gif')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n",
    "- The pale red dots depict the entire training set, while the solid red dots are the minibatches. \n",
    "- Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. \n",
    "- We can see that the loss gets smaller as the weights get closer to their true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae9390dff2a67ecfe7f518497ad1cfb27838edb064eb7518a1f306763d1e93e3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
