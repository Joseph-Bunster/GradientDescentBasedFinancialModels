{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **gradient** is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. \n",
    "- We call our process **gradient descent** because it uses the gradient to descend the loss curve towards a minimum. \n",
    "- **Stochastic** means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample code\n",
    "'''\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type=\"invscaling\"):\n",
    "    \"\"\" Computes Ordinary Least SquaresLinear Regression with Stochastic Gradient Descent as the optimization algorithm.\n",
    "        :param feature_array: array with all feature vectors used to train the model\n",
    "        :param target_array: array with all target vectors used to train the model\n",
    "        :param to_predict: feature vector that is not contained in the training set. Used to make a new prediction\n",
    "        :param learn_rate_type: algorithm used to set the learning rate at each iteration.\n",
    "        :return: Predicted cooking time for the vector to_predict and the R-squared of the model.\n",
    "\"\"\"    # Pipeline of transformations to apply to an estimator. First applies Standard Scaling to the feature array.\n",
    "    # Then, when the model is fitting the data it runs Stochastic Gradient Descent as the optimization algorithm.\n",
    "    # The estimator is always the last element.\n",
    "    \n",
    "    start_time = time.time()\n",
    "    linear_regression_pipeline = make_pipeline(StandardScaler(), SGDRegressor(learning_rate=learn_rate_type))\n",
    "    \n",
    "    linear_regression_pipeline.fit(feature_array, target_array)\n",
    "    stop_time = time.time()\n",
    "     \n",
    "    print(\"Total runtime: %.6fs\" % (stop_time - start_time))\n",
    "    print(\"Algorithm used to set the learning rate: \" + learn_rate_type)\n",
    "    print(\"Model Coeffiecients: \" + str(linear_regression_pipeline[1].coef_))\n",
    "    print(\"Number of iterations: \" + str(linear_regression_pipeline[1].n_iter_))    # Make a prediction for a feature vector not in the training set\n",
    "    prediction = np.round(linear_regression_pipeline.predict(to_predict), 0)[0]\n",
    "    print(\"Predicted cooking time: \" + str(prediction) + \" minutes\")    \n",
    "    r_squared = np.round(linear_regression_pipeline.score(feature_array, target_array).reshape(-1, 1)[0][0], 2)\n",
    "    print(\"R-squared: \" + str(r_squared))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = [[500, 80, 30, 10],\n",
    "                 [550, 75, 25, 0],\n",
    "                 [475, 90, 35, 20],\n",
    "                 [450, 80, 20,25],\n",
    "                 [465, 75, 30, 0],\n",
    "                 [525, 65, 40, 15],\n",
    "                 [400, 85, 33, 0],\n",
    "                 [500, 60, 30, 30],\n",
    "                 [435, 45, 25, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_array = [17, 11, 21, 23, 22, 15, 25, 18, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = [[510, 50, 35, 10]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.002207s\n",
      "Algorithm used to set the learning rate: invscaling\n",
      "Model Coeffiecients: [-3.44255436  1.64426741  0.28734788  1.10977756]\n",
      "Number of iterations: 248\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.9\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.002049s\n",
      "Algorithm used to set the learning rate: invscaling\n",
      "Model Coeffiecients: [-3.4334707   1.65460307  0.28030575  1.10948447]\n",
      "Number of iterations: 249\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.9\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0.001016s\n",
      "Algorithm used to set the learning rate: adaptive\n",
      "Model Coeffiecients: [-3.49580986  1.64628544  0.30943501  1.16532428]\n",
      "Number of iterations: 96\n",
      "Predicted cooking time: 13.0 minutes\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type=\"adaptive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "With the limitations of Gradient Descent in mind, Stochastic Gradient Descent emerged as a way to tackle performance issues and speed up the convergence in large datasets.\n",
    "\n",
    "Stochastic Gradient Descent is a probabilistic approximation of Gradient Descent. It is an approximation because, at each step, the algorithm calculates the gradient for one observation picked at random, instead of calculating the gradient for the entire dataset.\n",
    "'''\n",
    "src: https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loop(runs=3):\n",
    "    \"\"\" Repeatedly computes the gradient of a function\n",
    "        Computes the gradient given the starting points and then uses the result of the gradient to feed the next iteration, with new points.\n",
    "        Prints out the result of the function at each iteration\n",
    "        :param: runs: number of iterations to compute\n",
    "    \"\"\"    # starting points\n",
    "    x = np.array([1, 2, 3])\n",
    "    \n",
    "    # quadratic function, a parabola\n",
    "    y = x**2\n",
    "    \n",
    "    for run in range(0, runs):\n",
    "        print(\"Iter \" + str(run) + \": Y=\" + str(y))        # compute first derivative\n",
    "        x = np.gradient(y, 1)        # update the function output\n",
    "        y = x ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Y=[1 4 9]\n",
      "Iter 1: Y=[ 9. 16. 25.]\n",
      "Iter 2: Y=[49. 64. 81.]\n",
      "Iter 3: Y=[225. 256. 289.]\n",
      "Iter 4: Y=[ 961. 1024. 1089.]\n",
      "Iter 5: Y=[3969. 4096. 4225.]\n",
      "Iter 6: Y=[16129. 16384. 16641.]\n",
      "Iter 7: Y=[65025. 65536. 66049.]\n",
      "Iter 8: Y=[261121. 262144. 263169.]\n",
      "Iter 9: Y=[1046529. 1048576. 1050625.]\n"
     ]
    }
   ],
   "source": [
    "gradient_loop(runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Neural Network-Based SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the training data we have, we need two more things:\n",
    "- A **loss function** that measures how good the network's predictions are. it measures the disparity between the the target's true value and the value the model predicts.\n",
    "  -- A common loss function for regression problems is the **mean absolute error or MAE**. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
    "- An **optimizer** that can tell the network how to change its weights. The optimizer is an algorithm that adjusts the weights to minimize the loss. \n",
    "  -- Virtually all of the optimization algorithms used in deep learning belong to a family called **stochastic gradient descent.**  They are iterative algorithms that train a network in steps. One step of training goes like this:\n",
    "\n",
    "    * Sample some training data and run it through the network to make predictions.\n",
    "    * Measure the loss between the predictions and the true values.\n",
    "    * Finally, adjust the weights in a direction that makes the loss smaller.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/batch-sgd.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='img/batch-sgd.gif')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n",
    "- The pale red dots depict the entire training set, while the solid red dots are the minibatches. \n",
    "- Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. \n",
    "- We can see that the loss gets smaller as the weights get closer to their true values.\n",
    "\n",
    "\n",
    "It is also important to note that:\n",
    "- the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the **learning rate**. \n",
    "   * A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values. \n",
    "- The **learning rate** and the **size of the minibatches** are the two parameters that have the largest effect on how the SGD training proceeds. \n",
    "   * Their interaction is often subtle and the right choice for these parameters isn't always obvious. \n",
    "- **Adam** is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mae\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nb**: Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.879006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-02</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.213013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-05</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.811987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-06</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.906987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSI</td>\n",
       "      <td>1987-01-07</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.923013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Index        Date         Open         High          Low        Close  \\\n",
       "0   HSI  1986-12-31  2568.300049  2568.300049  2568.300049  2568.300049   \n",
       "1   HSI  1987-01-02  2540.100098  2540.100098  2540.100098  2540.100098   \n",
       "2   HSI  1987-01-05  2552.399902  2552.399902  2552.399902  2552.399902   \n",
       "3   HSI  1987-01-06  2583.899902  2583.899902  2583.899902  2583.899902   \n",
       "4   HSI  1987-01-07  2607.100098  2607.100098  2607.100098  2607.100098   \n",
       "\n",
       "     Adj Close  Volume    CloseUSD  \n",
       "0  2568.300049     0.0  333.879006  \n",
       "1  2540.100098     0.0  330.213013  \n",
       "2  2552.399902     0.0  331.811987  \n",
       "3  2583.899902     0.0  335.906987  \n",
       "4  2607.100098     0.0  338.923013  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('dataset/indexProcessed.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000001.SS', '399001.SZ', 'GDAXI', 'GSPTSE', 'HSI', 'IXIC',\n",
       "       'J203.JO', 'N100', 'N225', 'NSEI', 'NYA', 'SSMI', 'TWII'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "a = le.fit(df['Index'])\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.879006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-02</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.213013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-05</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.811987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-06</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.906987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-07</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.923013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104219</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>66054.921880</td>\n",
       "      <td>66812.453130</td>\n",
       "      <td>66022.976560</td>\n",
       "      <td>66076.679690</td>\n",
       "      <td>66076.679690</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4625.367578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104220</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-26</td>\n",
       "      <td>66076.679690</td>\n",
       "      <td>66446.367190</td>\n",
       "      <td>66030.351560</td>\n",
       "      <td>66108.226560</td>\n",
       "      <td>66108.226560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4627.575859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104221</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>66108.226560</td>\n",
       "      <td>66940.250000</td>\n",
       "      <td>66102.546880</td>\n",
       "      <td>66940.250000</td>\n",
       "      <td>66940.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4685.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104222</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>66940.250000</td>\n",
       "      <td>67726.562500</td>\n",
       "      <td>66794.609380</td>\n",
       "      <td>67554.859380</td>\n",
       "      <td>67554.859380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4728.840157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104223</th>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>67554.859380</td>\n",
       "      <td>68140.851560</td>\n",
       "      <td>67554.859380</td>\n",
       "      <td>67964.039060</td>\n",
       "      <td>67964.039060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4757.482734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104224 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Index        Date          Open          High           Low  \\\n",
       "0           4  1986-12-31   2568.300049   2568.300049   2568.300049   \n",
       "1           4  1987-01-02   2540.100098   2540.100098   2540.100098   \n",
       "2           4  1987-01-05   2552.399902   2552.399902   2552.399902   \n",
       "3           4  1987-01-06   2583.899902   2583.899902   2583.899902   \n",
       "4           4  1987-01-07   2607.100098   2607.100098   2607.100098   \n",
       "...       ...         ...           ...           ...           ...   \n",
       "104219      6  2021-05-25  66054.921880  66812.453130  66022.976560   \n",
       "104220      6  2021-05-26  66076.679690  66446.367190  66030.351560   \n",
       "104221      6  2021-05-27  66108.226560  66940.250000  66102.546880   \n",
       "104222      6  2021-05-28  66940.250000  67726.562500  66794.609380   \n",
       "104223      6  2021-05-31  67554.859380  68140.851560  67554.859380   \n",
       "\n",
       "               Close     Adj Close  Volume     CloseUSD  \n",
       "0        2568.300049   2568.300049     0.0   333.879006  \n",
       "1        2540.100098   2540.100098     0.0   330.213013  \n",
       "2        2552.399902   2552.399902     0.0   331.811987  \n",
       "3        2583.899902   2583.899902     0.0   335.906987  \n",
       "4        2607.100098   2607.100098     0.0   338.923013  \n",
       "...              ...           ...     ...          ...  \n",
       "104219  66076.679690  66076.679690     0.0  4625.367578  \n",
       "104220  66108.226560  66108.226560     0.0  4627.575859  \n",
       "104221  66940.250000  66940.250000     0.0  4685.817500  \n",
       "104222  67554.859380  67554.859380     0.0  4728.840157  \n",
       "104223  67964.039060  67964.039060     0.0  4757.482734  \n",
       "\n",
       "[104224 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Index'] = le.transform(df['Index'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the type of data from string to datetime that allows easy handling of time-series analysis. \n",
    "df = df.astype({'Date':'datetime64[ns]'})\n",
    "df['Year'] = df['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pull out the CloseUSD value based on the index and date\n",
    "'''\n",
    "def getPrice(x, df):\n",
    "    df = df.loc[(df['Index']  == x['Index'])  & (df['Date'] == x['Date'])]\n",
    "    if len(df['CloseUSD'])>0:\n",
    "        return df['CloseUSD'].values[0]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "      <th>Year</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.879006</td>\n",
       "      <td>1986</td>\n",
       "      <td>191.056328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-02</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.213013</td>\n",
       "      <td>1987</td>\n",
       "      <td>183.471992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-05</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.811987</td>\n",
       "      <td>1987</td>\n",
       "      <td>218.652793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-06</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.906987</td>\n",
       "      <td>1987</td>\n",
       "      <td>331.756328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1987-01-07</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.923013</td>\n",
       "      <td>1987</td>\n",
       "      <td>263.355352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index       Date         Open         High          Low        Close  \\\n",
       "0      4 1986-12-31  2568.300049  2568.300049  2568.300049  2568.300049   \n",
       "1      4 1987-01-02  2540.100098  2540.100098  2540.100098  2540.100098   \n",
       "2      4 1987-01-05  2552.399902  2552.399902  2552.399902  2552.399902   \n",
       "3      4 1987-01-06  2583.899902  2583.899902  2583.899902  2583.899902   \n",
       "4      4 1987-01-07  2607.100098  2607.100098  2607.100098  2607.100098   \n",
       "\n",
       "     Adj Close  Volume    CloseUSD  Year       Price  \n",
       "0  2568.300049     0.0  333.879006  1986  191.056328  \n",
       "1  2540.100098     0.0  330.213013  1987  183.471992  \n",
       "2  2552.399902     0.0  331.811987  1987  218.652793  \n",
       "3  2583.899902     0.0  335.906987  1987  331.756328  \n",
       "4  2607.100098     0.0  338.923013  1987  263.355352  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look for last day of an year for an Index\n",
    "df_last_date = df.groupby(['Index', 'Year']).agg({'Date':['max']})\n",
    "\n",
    "#reduce the column hierarachy to one.\n",
    "df_last_date.columns = df_last_date.columns.get_level_values(0)\n",
    "\n",
    "df_last_date.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "#Look for the price in the main df dataframe for last date of a year for an Index\n",
    "df['Price'] = df_last_date.apply(lambda x: getPrice(x, df), axis = 1)\n",
    "df.head(5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "      <th>Year</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>2568.300049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333.879006</td>\n",
       "      <td>1986</td>\n",
       "      <td>191.056328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>2540.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.213013</td>\n",
       "      <td>1987</td>\n",
       "      <td>183.471992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>2552.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.811987</td>\n",
       "      <td>1987</td>\n",
       "      <td>218.652793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>2583.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.906987</td>\n",
       "      <td>1987</td>\n",
       "      <td>331.756328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>2607.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>338.923013</td>\n",
       "      <td>1987</td>\n",
       "      <td>263.355352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index         Open         High          Low        Close    Adj Close  \\\n",
       "0      4  2568.300049  2568.300049  2568.300049  2568.300049  2568.300049   \n",
       "1      4  2540.100098  2540.100098  2540.100098  2540.100098  2540.100098   \n",
       "2      4  2552.399902  2552.399902  2552.399902  2552.399902  2552.399902   \n",
       "3      4  2583.899902  2583.899902  2583.899902  2583.899902  2583.899902   \n",
       "4      4  2607.100098  2607.100098  2607.100098  2607.100098  2607.100098   \n",
       "\n",
       "   Volume    CloseUSD  Year       Price  \n",
       "0     0.0  333.879006  1986  191.056328  \n",
       "1     0.0  330.213013  1987  183.471992  \n",
       "2     0.0  331.811987  1987  218.652793  \n",
       "3     0.0  335.906987  1987  331.756328  \n",
       "4     0.0  338.923013  1987  263.355352  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Date'])\n",
    "df.dropna(subset = [\"Price\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "      <th>Year</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>4</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.995000</td>\n",
       "      <td>1988</td>\n",
       "      <td>7477.029785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407.380994</td>\n",
       "      <td>1987</td>\n",
       "      <td>11481.472676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>4</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.160000</td>\n",
       "      <td>1988</td>\n",
       "      <td>372.290391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>4</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.881987</td>\n",
       "      <td>1988</td>\n",
       "      <td>528.690002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index         Open         High          Low        Close    Adj Close  \\\n",
       "366      4  2661.500000  2661.500000  2661.500000  2661.500000  2661.500000   \n",
       "113      4  3133.699951  3133.699951  3133.699951  3133.699951  3133.699951   \n",
       "426      4  2432.000000  2432.000000  2432.000000  2432.000000  2432.000000   \n",
       "320      4  2591.399902  2591.399902  2591.399902  2591.399902  2591.399902   \n",
       "\n",
       "     Volume    CloseUSD  Year         Price  \n",
       "366     0.0  345.995000  1988   7477.029785  \n",
       "113     0.0  407.380994  1987  11481.472676  \n",
       "426     0.0  316.160000  1988    372.290391  \n",
       "320     0.0  336.881987  1988    528.690002  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create training and validation splits\n",
    "df_train = df.sample(frac=0.7, random_state=0)\n",
    "df_valid = df.drop(df_train.index)\n",
    "display(df_train.head(4))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Scale to [0, 1]\n",
    "max_ = df_train.max(axis=0)\n",
    "min_ = df_train.min(axis=0)\n",
    "df_train = (df_train - min_) / (max_ - min_)\n",
    "df_valid = (df_valid - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X_train = df_train.drop('Price', axis=1)\n",
    "X_valid = df_valid.drop('Price', axis=1)\n",
    "\n",
    "y_train = df_train['Price']\n",
    "y_valid = df_valid['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>CloseUSD</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>4</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>2661.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.995000</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>4</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>3133.699951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407.380994</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>4</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>2432.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.160000</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>4</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>2591.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>336.881987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>2559.100098</td>\n",
       "      <td>2559.100098</td>\n",
       "      <td>2559.100098</td>\n",
       "      <td>2559.100098</td>\n",
       "      <td>2559.100098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>332.683013</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>4</td>\n",
       "      <td>2744.899902</td>\n",
       "      <td>2744.899902</td>\n",
       "      <td>2744.899902</td>\n",
       "      <td>2744.899902</td>\n",
       "      <td>2744.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>356.836987</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>4</td>\n",
       "      <td>3768.399902</td>\n",
       "      <td>3768.399902</td>\n",
       "      <td>3768.399902</td>\n",
       "      <td>3768.399902</td>\n",
       "      <td>3768.399902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.891987</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>4</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>2663.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>346.255000</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4</td>\n",
       "      <td>2659.899902</td>\n",
       "      <td>2659.899902</td>\n",
       "      <td>2659.899902</td>\n",
       "      <td>2659.899902</td>\n",
       "      <td>2659.899902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>345.786987</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>4</td>\n",
       "      <td>2509.699951</td>\n",
       "      <td>2509.699951</td>\n",
       "      <td>2488.199951</td>\n",
       "      <td>2488.199951</td>\n",
       "      <td>2488.199951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>323.465994</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index         Open         High          Low        Close    Adj Close  \\\n",
       "366      4  2661.500000  2661.500000  2661.500000  2661.500000  2661.500000   \n",
       "113      4  3133.699951  3133.699951  3133.699951  3133.699951  3133.699951   \n",
       "426      4  2432.000000  2432.000000  2432.000000  2432.000000  2432.000000   \n",
       "320      4  2591.399902  2591.399902  2591.399902  2591.399902  2591.399902   \n",
       "10       4  2559.100098  2559.100098  2559.100098  2559.100098  2559.100098   \n",
       "..     ...          ...          ...          ...          ...          ...   \n",
       "377      4  2744.899902  2744.899902  2744.899902  2744.899902  2744.899902   \n",
       "182      4  3768.399902  3768.399902  3768.399902  3768.399902  3768.399902   \n",
       "388      4  2663.500000  2663.500000  2663.500000  2663.500000  2663.500000   \n",
       "80       4  2659.899902  2659.899902  2659.899902  2659.899902  2659.899902   \n",
       "258      4  2509.699951  2509.699951  2488.199951  2488.199951  2488.199951   \n",
       "\n",
       "     Volume    CloseUSD  Year  \n",
       "366     0.0  345.995000  1988  \n",
       "113     0.0  407.380994  1987  \n",
       "426     0.0  316.160000  1988  \n",
       "320     0.0  336.881987  1988  \n",
       "10      0.0  332.683013  1987  \n",
       "..      ...         ...   ...  \n",
       "377     0.0  356.836987  1988  \n",
       "182     0.0  489.891987  1987  \n",
       "388     0.0  346.255000  1988  \n",
       "80      0.0  345.786987  1987  \n",
       "258     0.0  323.465994  1988  \n",
       "\n",
       "[304 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=[9]),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You'll learn more about model development in the exercises.\n",
    "\n",
    "After defining the model, we compile in the optimizer and loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we're ready to start the training! We've told Keras to feed the optimizer 256 rows of the training data at a time *(the batch_size)* and to do that 10 times all the way through the dataset *(the epochs*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train=np.asarray(X_train).astype(np.int)\n",
    "\n",
    "y_train=np.asarray(y_train).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000224C8FAA430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000224C8FAA430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000224C8FAA430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3196.0410WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000224CAD5A820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000224CAD5A820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000224CAD5A820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Constant constructor takes either 0 or 2 positional arguments\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 2s 1s/step - loss: 3144.7396 - val_loss: 2988.0894\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2854.2438 - val_loss: 2916.5952\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2764.9598 - val_loss: 2755.3657\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2690.3156 - val_loss: 2827.8323\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2738.1851 - val_loss: 2824.4353\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2716.2944 - val_loss: 2761.8418\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2707.8968 - val_loss: 2774.2485\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2732.8941 - val_loss: 2811.9160\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2710.0845 - val_loss: 2793.1318\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2744.3342 - val_loss: 2755.8770\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2691.6037 - val_loss: 2765.9087\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2708.2721 - val_loss: 2784.6045\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2679.6331 - val_loss: 2778.5376\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2684.9412 - val_loss: 2764.6699\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2668.1519 - val_loss: 2755.3357\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2699.3116 - val_loss: 2755.5522\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 2617.3729 - val_loss: 2758.2024\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2690.9403 - val_loss: 2753.6565\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2704.8975 - val_loss: 2756.0859\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2703.0096 - val_loss: 2763.1309\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 2737.1349 - val_loss: 2767.8154\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 2661.2708 - val_loss: 2768.3223\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 2731.5275 - val_loss: 2761.5396\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2696.5096 - val_loss: 2754.6768\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 2663.6914 - val_loss: 2753.3113\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 2664.5242 - val_loss: 2754.2434\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2674.2550 - val_loss: 2753.2371\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2692.0816 - val_loss: 2752.7759\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2702.8398 - val_loss: 2755.0916\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 2666.2888 - val_loss: 2763.1104\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2703.1312 - val_loss: 2767.2847\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2695.7062 - val_loss: 2761.4766\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2680.8021 - val_loss: 2754.1543\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2683.3116 - val_loss: 2752.4243\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2659.3167 - val_loss: 2754.1389\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 2675.7619 - val_loss: 2752.2837\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2705.2896 - val_loss: 2759.4290\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2631.7397 - val_loss: 2762.9170\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2674.4399 - val_loss: 2753.8171\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 2711.3434 - val_loss: 2753.2793\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 2688.7699 - val_loss: 2757.1528\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2734.7627 - val_loss: 2752.3142\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2665.4308 - val_loss: 2763.3518\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 2655.2997 - val_loss: 2771.6238\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2678.5378 - val_loss: 2762.3318\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2625.6934 - val_loss: 2751.8220\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2655.3779 - val_loss: 2757.0078\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2677.5780 - val_loss: 2760.9790\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 2659.1831 - val_loss: 2752.3928\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 2668.9242 - val_loss: 2757.3704\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see that Keras will keep you updated on the loss as the model trains.\n",
    "\n",
    "Often, a better way to view the loss though is to plot it. The **fit** method in fact keeps a record of the loss produced during training in a **History** object. We'll convert the data to a Pandas dataframe, which makes the plotting easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the training history to a dataframe\n",
    "history_df = pd.DataFrame(history.history)\n",
    "# use Pandas native plot method\n",
    "history_df['loss'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the loss levels off as the epochs go by. When the loss curve becomes horizontal like that, it means the model has learned all it can and there would be no reason continue for additional epochs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae9390dff2a67ecfe7f518497ad1cfb27838edb064eb7518a1f306763d1e93e3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
